
@string{aps = {GAN,}}


@article{einstein1950meaning,
  abbr={GAN},
  bibtex_show={true},
  title={HOW POWERFUL ARE GRAPH NEURAL NETWORKS?},
  link={https://arxiv.org/pdf/1810.00826.pdf},
  year={2019},
}
@string{aps = {American Physical Society,}}
References
@article{1,
  title={Large Language Models are Zero-Shot Reasoners},
  year={2023},
  url={https://arxiv.org/pdf/2205.11916.pdf}
}
@article{2,
  title={Orca 2: Teaching Small Language Models How to Reason},
  year={2023},
  url={https://arxiv.org/pdf/2311.11045.pdf}
}
@article{3,
  title={Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations},
  year={2019},
  url={http://cs.uccs.edu/~jkalita/work/reu/REU2019/19Griffith.pdf}
}
@article{4,
  title={Interpreting Deep Learning Models in Natural Language Processing: A Review},
  year={2021},
  url={https://www.semanticscholar.org/reader/d5784fd3ac7e06ec030abb8f7787faa9279c1a50}

@article {Huang2023.03.19.23287458,
	author = {Kexin Huang and Payal Chandak and Qianwen Wang and Shreyas Havaldar and Akhil Vaid and Jure Leskovec and Girish Nadkarni and Benjamin S. Glicksberg and Nils Gehlenborg and Marinka Zitnik},
	title = {Zero-shot drug repurposing with geometric deep learning and clinician centered design},
	elocation-id = {2023.03.19.23287458},
	year = {2023},
	doi = {10.1101/2023.03.19.23287458},
	publisher = {Cold Spring Harbor Laboratory Press},
	URL = {https://www.medrxiv.org/content/early/2023/09/28/2023.03.19.23287458},
	eprint = {https://www.medrxiv.org/content/early/2023/09/28/2023.03.19.23287458.full.pdf},
	journal = {medRxiv}
}

@article{Lin2022-kx,
  title    = "Evolutionary-scale prediction of atomic level protein structure
              with a language model",
  author   = "Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and
              Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil,
              Robert and Kabeli, Ori and Shmueli, Yaniv and dos Santos Costa,
              Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido,
              Salvatore and Rives, Alexander",
  abstract = "AbstractArtificial intelligence has the potential to open insight
              into the structure of proteins at the scale of evolution. It has
              only recently been possible to extend protein structure
              prediction to two hundred million cataloged proteins.
              Characterizing the structures of the exponentially growing
              billions of protein sequences revealed by large scale gene
              sequencing experiments would necessitate a break-through in the
              speed of folding. Here we show that direct inference of structure
              from primary sequence using a large language model enables an
              order of magnitude speed-up in high resolution structure
              prediction. Leveraging the insight that language models learn
              evolutionary patterns across millions of sequences, we train
              models up to 15B parameters, the largest language model of
              proteins to date. As the language models are scaled they learn
              information that enables prediction of the three-dimensional
              structure of a protein at the resolution of individual atoms.
              This results in prediction that is up to 60x faster than
              state-of-the-art while maintaining resolution and accuracy.
              Building on this, we present the ESM Metage-nomic Atlas. This is
              the first large-scale structural characterization of metagenomic
              proteins, with more than 617 million structures. The atlas
              reveals more than 225 million high confidence predictions,
              including millions whose structures are novel in comparison with
              experimentally determined structures, giving an unprecedented
              view into the vast breadth and diversity of the structures of
              some of the least understood proteins on earth.",
  journal  = "bioRxiv",
  month    =  jul,
  year     =  2022
}


@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}
@article{5,
  title={Salience Allocation as Guidance for Abstractive Summarization},
  year={2022},
  url={https://arxiv.org/pdf/2210.12330.pdf}
}
@article{6,
  title={CHARACTERIZING INTRINSIC COMPOSITIONALITY IN TRANSFORMERS WITH TREE PROJECTIONS},
  year={2022},
  url={https://arxiv.org/pdf/2211.01288.pdf}
}
@article{7,
  title={MATHPROMPTER: MATHEMATICAL REASONING USING LARGE LANGUAGE MODELS},
  year={2023},
  url={https://arxiv.org/pdf/2303.05398.pdf}
}
@article{8,
  title={TOWARDS HIERARCHICAL IMPORTANCE ATTRIBUTION: EXPLAINING COMPOSITIONAL SEMANTICS FOR NEURAL SEQUENCE MODELS},
  year={2020},
  url={https://arxiv.org/pdf/1911.06194.pdf}
}
@article{9,
  title={A survey of transformers},
  year={2022},
  url={https://www.sciencedirect.com/science/article/pii/S2666651022000146}
}
@article{10,
  title={Towards Reasoning in Large Language Models: A Survey},
  year={2023},
  url={https://arxiv.org/pdf/2212.10403.pdf}
}
@article{11,
  title={Google DeepMind’s new Gemini model looks amazing—but could signal peak AI hype},
  year={2023},
  url={https://www.technologyreview.com/2023/12/06/1084471/google-deepminds-new-gemini-model-looks-amazing-but-could-signal-peak-ai-hype/}
}
@article{12,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year={2022},
  url={https://openreview.net/pdf?id=_VjQlMeSB_J}
}
@article{13,
  title={Attention Is All You Need},
  year={2017},
  url={https://arxiv.org/pdf/1706.03762.pdf}
}
@article{14,
  title={AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS},
  year={2022},
  url={https://arxiv.org/pdf/2210.03493.pdf}
}
@article{15,
  title={From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?},
  year={2009},
  url={https://aclanthology.org/P09-2066.pdf}
}
