References
@article{1,
  title={Large Language Models are Zero-Shot Reasoners},
  year={2023},
  url={https://arxiv.org/pdf/2205.11916.pdf}
}
@article{2,
  title={Orca 2: Teaching Small Language Models How to Reason},
  year={2023},
  url={https://arxiv.org/pdf/2311.11045.pdf}
}
@article{3,
  title={Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations},
  year={2019},
  url={http://cs.uccs.edu/~jkalita/work/reu/REU2019/19Griffith.pdf}
}
@article{4,
  title={Interpreting Deep Learning Models in Natural Language Processing: A Review},
  year={2021},
  url={https://www.semanticscholar.org/reader/d5784fd3ac7e06ec030abb8f7787faa9279c1a50}
}
@article{5,
  title={Salience Allocation as Guidance for Abstractive Summarization},
  year={2022},
  url={https://arxiv.org/pdf/2210.12330.pdf}
}
@article{6,
  title={CHARACTERIZING INTRINSIC COMPOSITIONALITY IN TRANSFORMERS WITH TREE PROJECTIONS},
  year={2022},
  url={https://arxiv.org/pdf/2211.01288.pdf}
}
@article{7,
  title={MATHPROMPTER: MATHEMATICAL REASONING USING LARGE LANGUAGE MODELS},
  year={2023},
  url={https://arxiv.org/pdf/2303.05398.pdf}
}
@article{8,
  title={TOWARDS HIERARCHICAL IMPORTANCE ATTRIBUTION: EXPLAINING COMPOSITIONAL SEMANTICS FOR NEURAL SEQUENCE MODELS},
  year={2020},
  url={https://arxiv.org/pdf/1911.06194.pdf}
}
@article{9,
  title={A survey of transformers},
  year={2022},
  url={https://www.sciencedirect.com/science/article/pii/S2666651022000146}
}
@article{10,
  title={Towards Reasoning in Large Language Models: A Survey},
  year={2023},
  url={https://arxiv.org/pdf/2212.10403.pdf}
}
@article{11,
  title={Google DeepMind’s new Gemini model looks amazing—but could signal peak AI hype},
  year={2023},
  url={https://www.technologyreview.com/2023/12/06/1084471/google-deepminds-new-gemini-model-looks-amazing-but-could-signal-peak-ai-hype/}
}
@article{12,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year={2022},
  url={https://openreview.net/pdf?id=_VjQlMeSB_J}
}
@article{13,
  title={Attention Is All You Need},
  year={2017},
  url={https://arxiv.org/pdf/1706.03762.pdf}
}
@article{14,
  title={AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS},
  year={2022},
  url={https://arxiv.org/pdf/2210.03493.pdf}
}
@article{15,
  title={From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?},
  year={2009},
  url={https://aclanthology.org/P09-2066.pdf}
}
