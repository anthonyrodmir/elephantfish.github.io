
@misc{li_emergent_2023,
	title = {Emergent {World} {Representations}: {Exploring} a {Sequence} {Model} {Trained} on a {Synthetic} {Task}},
	shorttitle = {Emergent {World} {Representations}},
	url = {http://arxiv.org/abs/2210.13382},
	abstract = {Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
	month = feb,
	year = {2023},
	note = {arXiv:2210.13382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: ICLR 2023 oral (notable-top-5\%): https://openreview.net/forum?id=DeG07\_TcZvT ; code: https://github.com/likenneth/othello\_world},
	file = {arXiv.org Snapshot:/Users/tarark/Zotero/storage/VINCT7NU/2210.html:text/html;Full Text PDF:/Users/tarark/Zotero/storage/MYYIU6TM/Li et al. - 2023 - Emergent World Representations Exploring a Sequen.pdf:application/pdf},
}

@misc{elhage2022toy,
      title={Toy Models of Superposition}, 
      author={Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
      year={2022},
      eprint={2209.10652},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }


@misc{yun2023transformer,
      title={Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors}, 
      author={Zeyu Yun and Yubei Chen and Bruno A Olshausen and Yann LeCun},
      year={2023},
      eprint={2103.15949},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{bloom_decision_nodate,
	title = {Decision {Transformer} {Interpretability}},
	url = {https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability},
	abstract = {TLDR: We analyse how a small Decision Transformer learns to simulate agents on a grid world task, providing evidence that it is possible to do circui…},
	language = {en},
	urldate = {2023-11-09},
	author = {Bloom, Joseph and Colognese, Paul},
	file = {Snapshot:/Users/tarark/Zotero/storage/2RAE253V/decision-transformer-interpretability.html:text/html},
}

@misc{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	shorttitle = {Decision {Transformer}},
	url = {http://arxiv.org/abs/2106.01345},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01345 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: First two authors contributed equally. Last two authors advised equally},
	file = {arXiv.org Snapshot:/Users/tarark/Zotero/storage/LLGEXM4J/2106.html:text/html;Full Text PDF:/Users/tarark/Zotero/storage/PX4XHJBC/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf:application/pdf},
}

@misc{nanda_othello_2023,
 title={Actually, Othello-GPT Has A Linear Emergent World Model}, 
 url={<https://neelnanda.io/mechanistic-interpretability/othello>}, 
 journal={neelnanda.io}, author={Nanda, Neel}, year={2023}, month={Mar}
 }

 @misc{OthelloScope,
 author={Albert Garde, Esben Kran},
 url={https://kran.ai/othelloscope/index.html}}

 @misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/neelnanda-io/TransformerLens}},
}

@misc{dictionarylearning,
	author = {Sam Marks, Aaron Mueller},
	year = {2023},
	howpublished = {\url{https://www.alignmentforum.org/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning}}
	}

@misc{noauthor_improving_nodate,
	title = {Improving language understanding with unsupervised learning},
	url = {https://openai.com/research/language-unsupervised},
	abstract = {We’ve obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we’re also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets.},
	language = {en-US},
	urldate = {2023-12-13},
	file = {Snapshot:/Users/tarark/Zotero/storage/W8WXA78Y/language-unsupervised.html:text/html},
}
