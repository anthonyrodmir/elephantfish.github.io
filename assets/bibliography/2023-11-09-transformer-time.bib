@article{Zeng_Chen_Zhang_Xu_2023, 
title={Are Transformers Effective for Time Series Forecasting?}, 
volume={37}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/26317}, 
DOI={10.1609/aaai.v37i9.26317}, 
abstractNote={Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. 
Despite the growing performance over the past few years, 
we question the validity of this line of research in this work. Specifically, 
Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. 
However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. 
While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, 
the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. 
To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. 
Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, 
and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. 
We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.}, 
number={9}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang}, 
ear={2023}, month={Jun.}, 
pages={11121-11128} }

@article{Li2017DiffusionCR,
  title={Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting},
  author={Yaguang Li and Rose Yu and Cyrus Shahabi and Yan Liu},
  journal={arXiv: Learning},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:3508727}
}

@article{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021, 
title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
volume={35}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/17325}, 
DOI={10.1609/aaai.v35i12.17325}, 
abstractNote={Many real-world applications require the prediction of long sequence time-series, 
such as electricity consumption planning. 
Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, 
which is the ability to capture precise long-range dependency coupling between output and input efficiently. 
Recent studies have shown the potential of Transformer to increase the prediction capacity. 
However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, 
including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. 
To address these issues, we design an efficient transformer-based model for LSTF, named Informer, 
with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences’ dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai}, 
year={2021}, month={May}, pages={11106-11115} }

@article{https://doi.org/10.1111/tgis.12644,
author = {Cai, Ling and Janowicz, Krzysztof and Mai, Gengchen and Yan, Bo and Zhu, Rui},
title = {Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting},
journal = {Transactions in GIS},
volume = {24},
number = {3},
pages = {736-755},
doi = {https://doi.org/10.1111/tgis.12644},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.12644},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644},
abstract = {Abstract Traffic forecasting is a challenging problem due to the complexity of jointly modeling spatio-temporal dependencies at different scales. Recently, several hybrid deep learning models have been developed to capture such dependencies. These approaches typically utilize convolutional neural networks or graph neural networks (GNNs) to model spatial dependency and leverage recurrent neural networks (RNNs) to learn temporal dependency. However, RNNs are only able to capture sequential information in the time series, while being incapable of modeling their periodicity (e.g., weekly patterns). Moreover, RNNs are difficult to parallelize, making training and prediction less efficient. In this work we propose a novel deep learning architecture called Traffic Transformer to capture the continuity and periodicity of time series and to model spatial dependency. Our work takes inspiration from Google’s Transformer framework for machine translation. We conduct extensive experiments on two real-world traffic data sets, and the results demonstrate that our model outperforms baseline models by a substantial margin.},
year = {2020}
}

@article{DBLP:journals/corr/abs-2010-02803,
  author       = {George Zerveas and
                  Srideepika Jayaraman and
                  Dhaval Patel and
                  Anuradha Bhamidipaty and
                  Carsten Eickhoff},
  title        = {A Transformer-based Framework for Multivariate Time Series Representation
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2010.02803},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.02803},
  eprinttype    = {arXiv},
  eprint       = {2010.02803},
  timestamp    = {Mon, 15 Nov 2021 08:47:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-02803.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3534678.3539396, author = {Shao, Zezhi and Zhang, Zhao and Wang, Fei and Xu, Yongjun}, title = {Pre-Training Enhanced Spatial-Temporal Graph Neural Network for Multivariate Time Series Forecasting}, year = {2022}, isbn = {9781450393850}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3534678.3539396}, doi = {10.1145/3534678.3539396}, abstract = {Multivariate Time Series (MTS) forecasting plays a vital role in a wide range of applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have become increasingly popular MTS forecasting methods. STGNNs jointly model the spatial and temporal patterns of MTS through graph neural networks and sequential models, significantly improving the prediction accuracy. But limited by model complexity, most STGNNs only consider short-term historical MTS data, such as data over the past one hour. However, the patterns of time series and the dependencies between them (i.e., the temporal and spatial patterns) need to be analyzed based on long-term historical MTS data. To address this issue, we propose a novel framework, in which STGNN is Enhanced by a scalable time series Pre-training model (STEP). Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series. Experiments on three public real-world datasets demonstrate that our framework is capable of significantly enhancing downstream STGNNs, and our pre-training model aptly captures temporal patterns.}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {1567–1577}, numpages = {11}, keywords = {multivariate time series forecasting, spatial-temporal graph neural network, pre-training model}, 
location = {Washington DC, USA}, series = {KDD '22} }


@misc{nie2023time,
      title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers}, 
      author={Yuqi Nie and Nam H. Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
      year={2023},
      eprint={2211.14730},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{He_2022_CVPR,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}

@article{feichtenhofer2022masked,
  title={Masked autoencoders as spatiotemporal learners},
  author={Feichtenhofer, Christoph and Li, Yanghao and He, Kaiming and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={35946--35958},
  year={2022}
}

@inproceedings{cini2022filling,
    title={Filling the G\_ap\_s: Multivariate Time Series Imputation by Graph Neural Networks},
    author={Andrea Cini and Ivan Marisca and Cesare Alippi},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=kOu3-S3wJ7}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{cleveland1990stl,
  title={STL: A seasonal-trend decomposition},
  author={Cleveland, Robert B and Cleveland, William S and McRae, Jean E and Terpenning, Irma},
  journal={J. Off. Stat},
  volume={6},
  number={1},
  pages={3--73},
  year={1990}
}

@article{DBLP:journals/corr/abs-2010-11929,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  journal      = {CoRR},
  volume       = {abs/2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprinttype    = {arXiv},
  eprint       = {2010.11929},
  timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}