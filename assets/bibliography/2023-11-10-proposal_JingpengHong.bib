@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}

@article{10.1145/3285029,
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
title = {Deep Learning Based Recommender System: A Survey and New Perspectives},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3285029},
doi = {10.1145/3285029},
month = {feb},
articleno = {5},
numpages = {38},
keywords = {survey, deep learning, Recommender system}
}

@misc{hidasi2016sessionbased,
      title={Session-based Recommendations with Recurrent Neural Networks}, 
      author={Balázs Hidasi and Alexandros Karatzoglou and Linas Baltrunas and Domonkos Tikk},
      year={2016},
      eprint={1511.06939},
url = {https://doi.org/10.48550/arXiv.1511.06939},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kremer2014implementing,
  title={Implementing the “wisdom of the crowd”},
  author={Kremer, Ilan and Mansour, Yishay and Perry, Motty},
  journal={Journal of Political Economy},
  volume={122},
  number={5},
  pages={988--1012},
  year={2014},
  url={https://doi.org/10.1086/676597},
  publisher={University of Chicago Press Chicago, IL}
}

@article{che2018recommender,
  title={Recommender systems as mechanisms for social learning},
  author={Che, Yeon-Koo and H{\"o}rner, Johannes},
  journal={The Quarterly Journal of Economics},
  volume={133},
  number={2},
  pages={871--925},
  year={2018},
  publisher={Oxford University Press},
url={https://doi.org/10.1093/qje/qjx044},
}

@misc{ma2018entire,
      title={Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate}, 
      author={Xiao Ma and Liqin Zhao and Guan Huang and Zhi Wang and Zelin Hu and Xiaoqiang Zhu and Kun Gai},
      year={2018},
      eprint={1804.07931},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@ARTICLE{1423975,
  author={Adomavicius, G. and Tuzhilin, A.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions}, 
  year={2005},
  volume={17},
  number={6},
  pages={734-749},
  doi={10.1109/TKDE.2005.99}}
  
@inproceedings{10.1145/3523227.3547379,
author = {Wang, Yuyan and Tao, Long and Zhang, Xian Xing},
title = {Recommending for a Multi-Sided Marketplace with Heterogeneous Contents},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3547379},
doi = {10.1145/3523227.3547379},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {456–459},
numpages = {4},
keywords = {multi-sided marketplace, multi-objective recommendation},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks}
}

@inproceedings{10.1145/2988450.2988451,
author = {Dai, Hanjun and Wang, Yichen and Trivedi, Rakshit and Song, Le},
title = {Recurrent Coevolutionary Latent Feature Processes for Continuous-Time Recommendation},
year = {2016},
isbn = {9781450347952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988450.2988451},
doi = {10.1145/2988450.2988451},
booktitle = {Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
pages = {29–34},
numpages = {6},
location = {Boston, MA, USA},
series = {DLRS 2016}
}

@misc{tang2018personalized,
      title={Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding}, 
      author={Jiaxi Tang and Ke Wang},
      year={2018},
      eprint={1809.07426},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}


@inproceedings{10.1145/2959100.2959167,
author = {Hidasi, Bal\'{a}zs and Quadrana, Massimo and Karatzoglou, Alexandros and Tikk, Domonkos},
title = {Parallel Recurrent Neural Network Architectures for Feature-Rich Session-Based Recommendations},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959167},
doi = {10.1145/2959100.2959167},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {241–248},
numpages = {8},
keywords = {recurrent neural networks, recommender systems, deep learning, gated recurrent units, training strategies},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@inproceedings{10.1145/2740908.2742726,
author = {Sedhain, Suvash and Menon, Aditya Krishna and Sanner, Scott and Xie, Lexing},
title = {AutoRec: Autoencoders Meet Collaborative Filtering},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742726},
doi = {10.1145/2740908.2742726},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {111–112},
numpages = {2},
keywords = {collaborative filtering, recommender systems, autoencoders},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3077136.3080786,
author = {Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell},
title = {IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080786},
doi = {10.1145/3077136.3080786},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {515–524},
numpages = {10},
keywords = {web search, question answering, information retrieval, recommender systems, information retrieval models, adversarial training},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.5555/3172077.3172127,
author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
title = {DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1725–1731},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3219819.3220007,
author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H.},
title = {Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220007},
doi = {10.1145/3219819.3220007},
abstract = {Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1930–1939},
numpages = {10},
keywords = {multi-task learning, neural network, mixture of experts, recommendation system},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3219819.3220023,
author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
title = {XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220023},
doi = {10.1145/3219819.3220023},
abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1754–1763},
numpages = {10},
keywords = {neural network, factorization machines, deep learning, recommender systems, feature interactions},
location = {London, United Kingdom},
series = {KDD '18}
}
