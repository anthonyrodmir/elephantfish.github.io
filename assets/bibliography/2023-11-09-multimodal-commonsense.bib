@article{li2021,
  author       = {Xiang Lorraine Li and
                  Adhiguna Kuncoro and
                  Cyprien de Masson d'Autume and
                  Phil Blunsom and
                  Aida Nematzadeh},
  title        = {A Systematic Investigation of Commonsense Understanding in Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2111.00607},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.00607},
  eprinttype    = {arXiv},
  eprint       = {2111.00607},
  timestamp    = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-00607.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{saharia2022,
      title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
      author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},
      year={2022},
      eprint={2205.11487},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhao2023,
      title={Large Language Models as Commonsense Knowledge for Large-Scale Task Planning},
      author={Zirui Zhao and Wee Sun Lee and David Hsu},
      year={2023},
      eprint={2305.14078},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{huggingface,
      title={Hugging Face},
      url={https://huggingface.co/docs/transformers/index},
      journal={Huggingface}
}

@misc{zellers2019,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?},
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sap2019,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@misc{bisk2019,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language},
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xue2023modality,
      title={The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation},
      author={Zihui Xue and Zhengqi Gao and Sucheng Ren and Hang Zhao},
      year={2023},
      eprint={2206.06487},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{rajani2019explain,
      title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
      author={Nazneen Fatema Rajani and Bryan McCann and Caiming Xiong and Richard Socher},
      year={2019},
      eprint={1906.02361},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zong2023mcomet, title={McOmet: Multimodal Fusion Transformer for Physical Audiovisual Commonsense Reasoning}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25813}, DOI={10.1609/aaai.v37i5.25813}, abstractNote={Physical commonsense reasoning is essential for building reliable and interpretable AI systems, which involves a general understanding of the physical properties and affordances of everyday objects, how these objects can be manipulated, and how they interact with others. It is fundamentally a multi-modal task, as physical properties are manifested through multiple modalities, including vision and acoustics. In this work, we present a unified framework, named Multimodal Commonsense Transformer (MCOMET), for physical audiovisual commonsense reasoning. MCOMET has two intriguing properties: i) it fully mines higher-ordered temporal relationships across modalities (e.g., pairs, triplets, and quadruplets); and ii) it restricts the cross-modal flow through the feature collection and propagation mechanism along with tight fusion bottlenecks, forcing the model to attend the most relevant parts in each modality and suppressing the dissemination of noisy information. We evaluate our model on a very recent public benchmark, PACS. Results show that MCOMET significantly outperforms a variety of strong baselines, revealing powerful multi-modal commonsense reasoning capabilities.}, number={5}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zong, Daoming and Sun, Shiliang}, year={2023}, month={Jun.}, pages={6621-6629} }

@InProceedings{yu2022pacs,
author="Yu, Samuel
and Wu, Peter
and Liang, Paul Pu
and Salakhutdinov, Ruslan
and Morency, Louis-Philippe",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="PACS: A Dataset forÂ Physical Audiovisual CommonSense Reasoning",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="292--309",
abstract="In order for AI to be safely deployed in real-world scenarios such as hospitals, schools, and the workplace, it must be able to robustly reason about the physical world. Fundamental to this reasoning is physical common sense: understanding the physical properties and affordances of available objects, how they can be manipulated, and how they interact with other objects. Physical commonsense reasoning is fundamentally a multi-sensory task, since physical properties are manifested through multiple modalities - two of them being vision and acoustics. Our paper takes a step towards real-world physical commonsense reasoning by contributing PACS: the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. Our dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem. Using PACS, we evaluate multiple state-of-the-art models on our new challenging task. While some models show promising results ({\$}{\$}70{\backslash}{\%}{\$}{\$}70{\%}accuracy), they all fall short of human performance ({\$}{\$}95{\backslash}{\%}{\$}{\$}95{\%}accuracy). We conclude the paper by demonstrating the importance of multimodal reasoning and providing possible avenues for future research.",
isbn="978-3-031-19836-6"
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{singh2022flava,
      title={FLAVA: A Foundational Language And Vision Alignment Model},
      author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
      year={2022},
      eprint={2112.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{radford2021clip,
      title={Learning Transferable Visual Models From Natural Language Supervision},
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jia2021align,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{koupaee2018wikihow,
      title={WikiHow: A Large Scale Text Summarization Dataset},
      author={Mahnaz Koupaee and William Yang Wang},
      year={2018},
      eprint={1810.09305},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}