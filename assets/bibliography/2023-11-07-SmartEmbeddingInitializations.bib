@article{Li2023,
  title = {On the Initialization of Graph Neural Networks}, 
  author = {Jiahang Li and Yakun Song and Xiang Song and David Paul Wipf},
  year = {2023},
  eprint = {2312.02622},
  archivePrefix = {arXiv},
  URL = {https://proceedings.mlr.press/v202/li23y.html},
  primaryClass = {cs.LG}
}

@article {Huang2023,
	author = {Kexin Huang and Payal Chandak and Qianwen Wang and Shreyas Havaldar and Akhil Vaid and Jure Leskovec and Girish Nadkarni and Benjamin S. Glicksberg and Nils Gehlenborg and Marinka Zitnik},
	title = {Zero-shot drug repurposing with geometric deep learning and clinician centered design},
	elocation-id = {2023.03.19.23287458},
	year = {2023},
	doi = {10.1101/2023.03.19.23287458},
	publisher = {Cold Spring Harbor Laboratory Press},
	URL = {https://www.medrxiv.org/content/early/2023/09/28/2023.03.19.23287458},
	eprint = {https://www.medrxiv.org/content/early/2023/09/28/2023.03.19.23287458.full.pdf},
	journal = {medRxiv}
}


@InProceedings{pmlr-v9-glorot10a,
  title = {Understanding the difficulty of training deep feedforward neural networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = {249--256},
  year = {2010},
  editor = {Teh, Yee Whye and Titterington, Mike},
  volume = {9},
  series = {Proceedings of Machine Learning Research},
  address = {Chia Laguna Resort, Sardinia, Italy},
  month = {13--15 May},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{Cui2021,
  author  = {Hejie Cui and Zijie Lu and Pan Li and Carl Yang},
  title = {On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs},
  journal = {CoRR},
  volume = {abs/2107.01495},
  year = {2021},
  url = {https://arxiv.org/abs/2107.01495},
  eprinttype = {arXiv},
  eprint = {2107.01495},
  timestamp = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-2107-01495.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Chandak2023,
  author={Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
  title={Building a knowledge graph to enable precision medicine},
  journal={Scientific Data},
  year={2023},
  month={Feb},
  day={02},
  volume={10},
  number={1},
  pages={67},
  issn={2052-4463},
  doi={10.1038/s41597-023-01960-3},
  url={https://doi.org/10.1038/s41597-023-01960-3}
}

@article{Lin2022-esm2,
  title    = {Evolutionary-scale prediction of atomic level protein structure with a language model},
  author   = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
  journal  = {bioRxiv},
  month    = {jul},
  year     = {2022}
}

@article{Gene_Ontology_Consortium2021-uk,
  title     = {The Gene Ontology resource: enriching a (GOld) mine},
  author    = {Gene Ontology Consortium},
  journal   = {Nucleic Acids Res.},
  publisher = {Oxford University Press (OUP)},
  volume    =  {49},
  number    = {D1},
  pages     = {D325--D334},
  month     =  {jan},
  year      =  {2021},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@ARTICLE{Yang2014-zb,
  title = {Embedding entities and relations for learning and inference in knowledge bases},
  author = {Yang, Bishan and Yih, Wen-Tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  publisher = {arXiv},
  year = {2014},
  doi = {https://doi.org/10.48550/arXiv.1412.6575}
}

