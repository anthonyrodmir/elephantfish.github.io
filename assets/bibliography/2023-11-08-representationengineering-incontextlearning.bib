@article{berglund2023measuring,
  title={On measuring situational awareness in LLMs},
  author={Berglund, Lukas and et al.},
  journal={arXiv preprint, arXiv:2309.00667},
  year={2023},
  url={https://arxiv.org/pdf/2309.00667.pdf}
}

@article{zou2023representation,
  title={REPRESENTATION ENGINEERING: A TOP-DOWN APPROACH TO AI TRANSPARENCY},
  author={Zou, Andy and et al.},
  journal={arXiv preprint, arXiv:2310.01405},
  year={2023},
  url={https://arxiv.org/pdf/2310.01405.pdf}
}

@article{meng2023massediting,
  title={MASS-EDITING MEMORY IN A TRANSFORMER},
  author={Meng, Kevin and et al.},
  journal={arXiv preprint, arXiv:2210.07229},
  year={2023},
  url={https://arxiv.org/pdf/2210.07229.pdf}
}

@article{meng2023locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and et al.},
  journal={arXiv preprint, arXiv:2202.05262},
  year={2023},
  url={https://arxiv.org/pdf/2202.05262.pdf}
}




@misc{zhou2022teaching,
      title={Teaching Algorithmic Reasoning via In-context Learning}, 
      author={Hattie Zhou and Azade Nova and Hugo Larochelle and Aaron Courville and Behnam Neyshabur and Hanie Sedghi},
      year={2022},
      eprint={2211.09066},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kojima2023stepbystep,
      title={Language Models are Few-Shot Learners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2023deepbreath,
      title={Large Language Models as SOptimizers}, 
      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
      year={2023},
      eprint={2309.03409},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{min2022rethinking,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023larger,
      title={Larger language models do in-context learning differently}, 
      author={Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
      year={2023},
      eprint={2303.03846},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xie2022explanation,
      title={An Explanation of In-context Learning as Implicit Bayesian Inference}, 
      author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
      year={2022},
      eprint={2111.02080},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{zhong2023mquake,
      title={MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions}, 
      author={Zexuan Zhong and Zhengxuan Wu and Christopher D. Manning and Christopher Potts and Danqi Chen},
      year={2023},
      eprint={2305.14795},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{hernandez2023inspecting,
      title={Inspecting and Editing Knowledge Representations in Language Models}, 
      author={Evan Hernandez and Belinda Z. Li and Jacob Andreas},
      year={2023},
      eprint={2304.00740},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shao2023gold,
      title={Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information}, 
      author={Shun Shao and Yftah Ziser and Shay B. Cohen},
      year={2023},
      eprint={2203.07893},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{belrose2023leace,
      title={LEACE: Perfect linear concept erasure in closed form}, 
      author={Nora Belrose and David Schneider-Joseph and Shauli Ravfogel and Ryan Cotterell and Edward Raff and Stella Biderman},
      year={2023},
      eprint={2306.03819},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2023inferencetime,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2023},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{akyürek2023learning,
      title={What learning algorithm is in-context learning? Investigations with linear models}, 
      author={Ekin Akyürek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
      year={2023},
      eprint={2211.15661},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{vonoswald2023transformers,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2023},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}
