@unpublished{Dosovitskiy2020,
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	keywords = {befreqy},
	month = oct,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	year = 2020}

@misc{AttentionIsAllYouNeed,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani,  Ashish and Shazeer,  Noam and Parmar,  Niki and Uszkoreit,  Jakob and Jones,  Llion and Gomez,  Aidan N. and Kaiser,  Lukasz and Polosukhin,  Illia},
  keywords = {Computation and Language (cs.CL),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{low-pass,
  doi = {10.48550/ARXIV.2202.06709},
  url = {https://arxiv.org/abs/2202.06709},
  author = {Park,  Namuk and Kim,  Songkuk},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {How Do Vision Transformers Work?},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Grechishnikova2021,
	author = {Grechishnikova, Daria},
	journal = {Sci. Rep.},
	language = {en},
	month = jan,
	number = 1,
	pages = {321},
	title = {Transformer neural network for protein-specific de novo drug generation as a machine translation problem},
	volume = 11,
	year = 2021}



@article{Hu2022,
	author = {Hu, Rui and Chen, Jie and Zhou, Li},
	journal = {Comput. Biol. Med.},
	keywords = {Arrhythmia detection; Deep learning; ECG classification; Signal processing; Transformer;dl},
	language = {en},
	month = may,
	pages = {105325},
	title = {A transformer-based deep neural network for arrhythmia detection using continuous {ECG} signals},
	volume = 144,
	year = 2022}


@article{Wang2022,
	archiveprefix = {arXiv},
	author = {Wang, Peihao and Zheng, Wenqing and Chen, Tianlong and Wang, Zhangyang},
	eprint = {2203.05962},
	month = mar,
	primaryclass = {cs.CV},
	title = {{Anti-Oversmoothing} in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice},
	year = 2022}



@article{Liu2021,
	archiveprefix = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	eprint = {2103.14030},
	keywords = {befreqy},
	month = mar,
	pages = {10012--10022},
	primaryclass = {cs.CV},
	title = {Swin Transformer: Hierarchical vision Transformer using shifted windows},
	year = 2021}

@inproceedings{Liang2021,
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	booktitle = {Proceedings of the {IEEE/CVF} international conference on computer vision},
	keywords = {befreqy},
	pages = {1833--1844},
	title = {Swinir: Image restoration using swin transformer},
	year = 2021}


@article{Li2022,
	archiveprefix = {arXiv},
	author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
	eprint = {2203.16527},
	month = mar,
	primaryclass = {cs.CV},
	title = {Exploring Plain Vision Transformer Backbones for Object Detection},
	year = 2022}



@article{Naimi2021,
	archiveprefix = {arXiv},
	author = {Naimi, Safwen and van Leeuwen, Rien and Souidene, Wided and Ben Saoud, Slim},
	eprint = {2111.04845},
	month = nov,
	primaryclass = {cs.CV},
	title = {Hybrid {BYOL-ViT}: Efficient approach to deal with small datasets},
	year = 2021}



@article{Zhu2023,
	author = {Zhu, Haoran and Chen, Boyuan and Yang, Carter},
	month = feb,
	title = {Understanding Why {ViT} Trains Badly on Small Datasets: An Intuitive Perspective},
	year = 2023}


@article{Park2022,
	author = {Park, Namuk and Kim, Songkuk},
	month = feb,
	title = {How Do Vision Transformers Work?},
	year = 2022}

@article{Si2022,
	author = {Si, Chenyang and Yu, Weihao and Zhou, Pan and Zhou, Yichen and Wang, Xinchao and Yan, Shuicheng},
	month = may,
	pages = {23495--23509},
	title = {Inception Transformer},
	year = 2022}

@article{Liu2020,
	author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
	month = apr,
	title = {Understanding the Difficulty of Training Transformers},
	year = 2020}


@article{He2023,
	author = {He, Bobby and Hofmann, Thomas},
	month = nov,
	title = {Simplifying Transformer Blocks},
	year = 2023}


@article{Trockman2023,
	author = {Trockman, Asher and Zico Kolter, J},
	month = may,
	title = {Mimetic Initialization of {Self-Attention} Layers},
	year = 2023}

@article{ConvViT,
	Author = {St√©phane d'Ascoli and Hugo Touvron and Matthew Leavitt and Ari Morcos and Giulio Biroli and Levent Sagun},
	Title = {ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases},
	Year = {2021},
	Eprint = {arXiv:2103.10697},
	Doi = {10.1088/1742-5468/ac9830},
	}

@misc{2306.01610,
	Author = {Ameen Ali and Tomer Galanti and Lior Wolf},
	Title = {Centered Self-Attention Layers},
	Year = {2023},
	Eprint = {arXiv:2306.01610},
	}
@misc{2302.05442,
Author = {Dehghani M. et al},
Title = {Scaling Vision Transformers to 22 Billion Parameters},
Year = {2023},
Eprint = {arXiv:2302.05442},
}
