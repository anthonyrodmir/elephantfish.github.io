
@misc{huang_towards_2023,
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Towards {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.10403},
	doi = {10.48550/arXiv.2212.10403},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = may,
	year = {2023},
	note = {arXiv:2212.10403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ACL 2023 Findings, 15 pages},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/EUINZVEN/Huang and Chang - 2023 - Towards Reasoning in Large Language Models A Surv.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/8YFN9VTL/2212.html:text/html},
}

@misc{mitra_how_2020,
	title = {How {Additional} {Knowledge} can {Improve} {Natural} {Language} {Commonsense} {Question} {Answering}?},
	url = {http://arxiv.org/abs/1909.08855},
	doi = {10.48550/arXiv.1909.08855},
	abstract = {Recently several datasets have been proposed to encourage research in Question Answering domains where commonsense knowledge is expected to play an important role. Recent language models such as ROBERTA, BERT and GPT that have been pre-trained on Wikipedia articles and books have shown reasonable performance with little fine-tuning on several such Multiple Choice Question-Answering (MCQ) datasets. Our goal in this work is to develop methods to incorporate additional (commonsense) knowledge into language model-based approaches for better question-answering in such domains. In this work, we first categorize external knowledge sources, and show performance does improve on using such sources. We then explore three different strategies for knowledge incorporation and four different models for question-answering using external commonsense knowledge. We analyze our predictions to explore the scope of further improvements.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Mitra, Arindam and Banerjee, Pratyay and Pal, Kuntal Kumar and Mishra, Swaroop and Baral, Chitta},
	month = apr,
	year = {2020},
	note = {arXiv:1909.08855 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 14 pages, 14 figures, 3 tables},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/JSW9KZQD/Mitra et al. - 2020 - How Additional Knowledge can Improve Natural Langu.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/QERCRKYA/1909.html:text/html},
}

@misc{davis_benchmarks_2023,
	title = {Benchmarks for {Automated} {Commonsense} {Reasoning}: {A} {Survey}},
	shorttitle = {Benchmarks for {Automated} {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2302.04752},
	doi = {10.48550/arXiv.2302.04752},
	abstract = {More than one hundred benchmarks have been developed to test the commonsense knowledge and commonsense reasoning abilities of artificial intelligence (AI) systems. However, these benchmarks are often flawed and many aspects of common sense remain untested. Consequently, we do not currently have any reliable way of measuring to what extent existing AI systems have achieved these abilities. This paper surveys the development and uses of AI commonsense benchmarks. We discuss the nature of common sense; the role of common sense in AI; the goals served by constructing commonsense benchmarks; and desirable features of commonsense benchmarks. We analyze the common flaws in benchmarks, and we argue that it is worthwhile to invest the work needed ensure that benchmark examples are consistently high quality. We survey the various methods of constructing commonsense benchmarks. We enumerate 139 commonsense benchmarks that have been developed: 102 text-based, 18 image-based, 12 video based, and 7 simulated physical environments. We discuss the gaps in the existing benchmarks and aspects of commonsense reasoning that are not addressed in any existing benchmark. We conclude with a number of recommendations for future development of commonsense AI benchmarks.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Davis, Ernest},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04752 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/KHHCYDCI/Davis - 2023 - Benchmarks for Automated Commonsense Reasoning A .pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/QXYS2ADY/2302.html:text/html},
}

@inproceedings{boratko_protoqa_2020,
	address = {Online},
	title = {{ProtoQA}: {A} {Question} {Answering} {Dataset} for {Prototypical} {Common}-{Sense} {Reasoning}},
	shorttitle = {{ProtoQA}},
	url = {https://aclanthology.org/2020.emnlp-main.85},
	doi = {10.18653/v1/2020.emnlp-main.85},
	abstract = {Given questions regarding some prototypical situation — such as Name something that people usually do before they leave the house for work? — a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show – Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Boratko, Michael and Li, Xiang and O'Gorman, Tim and Das, Rajarshi and Le, Dan and McCallum, Andrew},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {1122--1136},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/7LUK4AC8/Boratko et al. - 2020 - ProtoQA A Question Answering Dataset for Prototyp.pdf:application/pdf},
}

@inproceedings{hovy_importance_2021,
	address = {Online},
	title = {The {Importance} of {Modeling} {Social} {Factors} of {Language}: {Theory} and {Practice}},
	shorttitle = {The {Importance} of {Modeling} {Social} {Factors} of {Language}},
	url = {https://aclanthology.org/2021.naacl-main.49},
	doi = {10.18653/v1/2021.naacl-main.49},
	abstract = {Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language's social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Yang, Diyi},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {588--602},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/FN8SBV4Z/Hovy and Yang - 2021 - The Importance of Modeling Social Factors of Langu.pdf:application/pdf},
}

@misc{forbes_social_2021,
	title = {Social {Chemistry} 101: {Learning} to {Reason} about {Social} and {Moral} {Norms}},
	shorttitle = {Social {Chemistry} 101},
	url = {http://arxiv.org/abs/2011.00620},
	doi = {10.48550/arXiv.2011.00620},
	abstract = {Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes." We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Forbes, Maxwell and Hwang, Jena D. and Shwartz, Vered and Sap, Maarten and Choi, Yejin},
	month = aug,
	year = {2021},
	note = {arXiv:2011.00620 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/2NDCVH6E/Forbes et al. - 2021 - Social Chemistry 101 Learning to Reason about Soc.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/5JQIHLGP/2011.html:text/html},
}

@inproceedings{zellers_hellaswag_2019,
	address = {Florence, Italy},
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {https://aclanthology.org/P19-1472},
	doi = {10.18653/v1/P19-1472},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textbackslash}textgreater95\% accuracy), state-of-the-art models struggle ({\textbackslash}textless48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical `Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {4791--4800},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/8B9TEHT3/Zellers et al. - 2019 - HellaSwag Can a Machine Really Finish Your Senten.pdf:application/pdf},
}

@misc{nematzadeh_evaluating_2018,
	title = {Evaluating {Theory} of {Mind} in {Question} {Answering}},
	url = {http://arxiv.org/abs/1808.09352},
	doi = {10.48550/arXiv.1808.09352},
	abstract = {We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Nematzadeh, Aida and Burns, Kaylee and Grant, Erin and Gopnik, Alison and Griffiths, Thomas L.},
	month = aug,
	year = {2018},
	note = {arXiv:1808.09352 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/B37NXFRQ/Nematzadeh et al. - 2018 - Evaluating Theory of Mind in Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/X82TZAL6/1808.html:text/html},
}

@misc{wang_semantic_2021,
	title = {Semantic {Categorization} of {Social} {Knowledge} for {Commonsense} {Question} {Answering}},
	url = {http://arxiv.org/abs/2109.05168},
	doi = {10.48550/arXiv.2109.05168},
	abstract = {Large pre-trained language models (PLMs) have led to great success on various commonsense question answering (QA) tasks in an end-to-end fashion. However, little attention has been paid to what commonsense knowledge is needed to deeply characterize these QA tasks. In this work, we proposed to categorize the semantics needed for these tasks using the SocialIQA as an example. Building upon our labeled social knowledge categories dataset on top of SocialIQA, we further train neural QA models to incorporate such social knowledge categories and relation information from a knowledge base. Unlike previous work, we observe our models with semantic categorizations of social knowledge can achieve comparable performance with a relatively simple model and smaller size compared to other complex approaches.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Wang, Gengyu and Hou, Xiaochen and Yang, Diyi and McKeown, Kathleen and Huang, Jing},
	month = sep,
	year = {2021},
	note = {arXiv:2109.05168 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by SustaiNLP 2021 on EMNLP 2021},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/D9BII9EH/Wang et al. - 2021 - Semantic Categorization of Social Knowledge for Co.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/KYTMCG2U/2109.html:text/html},
}

@inproceedings{chang_incorporating_2020,
	address = {Online},
	title = {Incorporating {Commonsense} {Knowledge} {Graph} in {Pretrained} {Models} for {Social} {Commonsense} {Tasks}},
	url = {https://aclanthology.org/2020.deelio-1.9},
	doi = {10.18653/v1/2020.deelio-1.9},
	abstract = {Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of {Deep} {Learning} {Inside} {Out} ({DeeLIO}): {The} {First} {Workshop} on {Knowledge} {Extraction} and {Integration} for {Deep} {Learning} {Architectures}},
	publisher = {Association for Computational Linguistics},
	author = {Chang, Ting-Yun and Liu, Yang and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Zhou, Pei and Hakkani-Tur, Dilek},
	editor = {Agirre, Eneko and Apidianaki, Marianna and Vulić, Ivan},
	month = nov,
	year = {2020},
	pages = {74--79},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/UI2QXD64/Chang et al. - 2020 - Incorporating Commonsense Knowledge Graph in Pretr.pdf:application/pdf},
}

@misc{gandhi_understanding_2023,
	title = {Understanding {Social} {Reasoning} in {Language} {Models} with {Language} {Models}},
	url = {http://arxiv.org/abs/2306.15448},
	abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15448 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/ninalei/Zotero/storage/VVQKYVP7/2306.html:text/html;Full Text PDF:/Users/ninalei/Zotero/storage/7BWUSP5N/Gandhi et al. - 2023 - Understanding Social Reasoning in Language Models .pdf:application/pdf},
}

@misc{li_systematic_2022,
	title = {A {Systematic} {Investigation} of {Commonsense} {Knowledge} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2111.00607},
	doi = {10.48550/arXiv.2111.00607},
	abstract = {Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -- a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Li, Xiang Lorraine and Kuncoro, Adhiguna and Hoffmann, Jordan and d'Autume, Cyprien de Masson and Blunsom, Phil and Nematzadeh, Aida},
	month = oct,
	year = {2022},
	note = {arXiv:2111.00607 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2022},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/8GL5SURD/Li et al. - 2022 - A Systematic Investigation of Commonsense Knowledg.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/R3ANLN9I/2111.html:text/html},
}

@misc{sap_neural_2023,
	title = {Neural {Theory}-of-{Mind}? {On} the {Limits} of {Social} {Intelligence} in {Large} {LMs}},
	shorttitle = {Neural {Theory}-of-{Mind}?},
	url = {http://arxiv.org/abs/2210.13312},
	doi = {10.48550/arXiv.2210.13312},
	abstract = {Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55\% and 60\% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind. In our updated version, we also analyze newer instruction tuned and RLFH models for neural ToM. We find that even ChatGPT and GPT-4 do not display emergent Theory of Mind; strikingly even GPT-4 performs only 60\% accuracy on the ToMi questions related to mental states and realities.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2210.13312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Originally published at EMNLP 2022, extended to include ChatGPT and GPT-4 models on March 30th 2023 (extension not peer reviewed)},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/7WCU8CD6/Sap et al. - 2023 - Neural Theory-of-Mind On the Limits of Social Int.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/VE2T9KMG/2210.html:text/html},
}

@misc{ziems_normbank_2023,
	title = {{NormBank}: {A} {Knowledge} {Bank} of {Situational} {Social} {Norms}},
	shorttitle = {{NormBank}},
	url = {http://arxiv.org/abs/2305.17008},
	doi = {10.48550/arXiv.2305.17008},
	abstract = {We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NormBank contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic - one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NormBank. We further demonstrate the utility of this resource with a series of transfer experiments.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Ziems, Caleb and Dwivedi-Yu, Jane and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi},
	month = jul,
	year = {2023},
	note = {arXiv:2305.17008 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/JYTX6KM8/Ziems et al. - 2023 - NormBank A Knowledge Bank of Situational Social N.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/SFX4K5PX/2305.html:text/html},
}

@misc{sap_socialiqa_2019,
	title = {{SocialIQA}: {Commonsense} {Reasoning} about {Social} {Interactions}},
	shorttitle = {{SocialIQA}},
	url = {http://arxiv.org/abs/1904.09728},
	doi = {10.48550/arXiv.1904.09728},
	abstract = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
	month = sep,
	year = {2019},
	note = {arXiv:1904.09728 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: the first two authors contributed equally; accepted to EMNLP 2019; camera ready version},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/BJVQKQJH/Sap et al. - 2019 - SocialIQA Commonsense Reasoning about Social Inte.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/SEAIQEKC/1904.html:text/html},
}

@inproceedings{rondeau_systematic_2018,
	address = {Melbourne, Australia},
	title = {Systematic {Error} {Analysis} of the {Stanford} {Question} {Answering} {Dataset}},
	url = {https://aclanthology.org/W18-2602},
	doi = {10.18653/v1/W18-2602},
	abstract = {We analyzed the outputs of multiple question answering (QA) models applied to the Stanford Question Answering Dataset (SQuAD) to identify the core challenges for QA systems on this data set. Through an iterative process, challenging aspects were hypothesized through qualitative analysis of the common error cases. A classifier was then constructed to predict whether SQuAD test examples were likely to be difficult for systems to answer based on features associated with the hypothesized aspects. The classifier's performance was used to accept or reject each aspect as an indicator of difficulty. With this approach, we ensured that our hypotheses were systematically tested and not simply accepted based on our pre-existing biases. Our explanations are not accepted based on human evaluation of individual examples. This process also enabled us to identify the primary QA strategy learned by the models, i.e., systems determined the acceptable answer type for a question and then selected the acceptable answer span of that type containing the highest density of words present in the question within its local vicinity in the passage.},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Rondeau, Marc-Antoine and Hazen, T. J.},
	editor = {Choi, Eunsol and Seo, Minjoon and Chen, Danqi and Jia, Robin and Berant, Jonathan},
	month = jul,
	year = {2018},
	pages = {12--20},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/K5VCCH83/Rondeau and Hazen - 2018 - Systematic Error Analysis of the Stanford Question.pdf:application/pdf},
}

@article{moldovan_performance_2003,
	title = {Performance issues and error analysis in an open-domain question answering system},
	volume = {21},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/763693.763694},
	doi = {10.1145/763693.763694},
	abstract = {This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various retrieval strategies and lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.},
	language = {en},
	number = {2},
	urldate = {2023-12-11},
	journal = {ACM Transactions on Information Systems},
	author = {Moldovan, Dan and Paşca, Marius and Harabagiu, Sanda and Surdeanu, Mihai},
	month = apr,
	year = {2003},
	pages = {133--154},
	file = {Full Text PDF:/Users/ninalei/Zotero/storage/HWCT35VT/Moldovan et al. - 2003 - Performance issues and error analysis in an open-d.pdf:application/pdf},
}

@misc{gao_towards_2022,
	title = {Towards {Automated} {Error} {Analysis}: {Learning} to {Characterize} {Errors}},
	shorttitle = {Towards {Automated} {Error} {Analysis}},
	url = {http://arxiv.org/abs/2201.05017},
	abstract = {Characterizing the patterns of errors that a system makes helps researchers focus future development on increasing its accuracy and robustness. We propose a novel form of "meta learning" that automatically learns interpretable rules that characterize the types of errors that a system makes, and demonstrate these rules' ability to help understand and improve two NLP systems. Our approach works by collecting error cases on validation data, extracting meta-features describing these samples, and finally learning rules that characterize errors using these features. We apply our approach to VilBERT, for Visual Question Answering, and RoBERTa, for Common Sense Question Answering. Our system learns interpretable rules that provide insights into systemic errors these systems make on the given tasks. Using these insights, we are also able to "close the loop" and modestly improve performance of these systems.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Gao, Tong and Singh, Shivang and Mooney, Raymond J.},
	month = feb,
	year = {2022},
	note = {arXiv:2201.05017 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 12 pages, 11 figures},
	file = {arXiv.org Snapshot:/Users/ninalei/Zotero/storage/EDFYTFTI/2201.html:text/html;Full Text PDF:/Users/ninalei/Zotero/storage/3TFZVLED/Gao et al. - 2022 - Towards Automated Error Analysis Learning to Char.pdf:application/pdf},
}

@misc{naveed_comprehensive_2023,
	title = {A {Comprehensive} {Overview} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	month = nov,
	year = {2023},
	note = {arXiv:2307.06435 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Work in-progress},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/E26GH9KA/Naveed et al. - 2023 - A Comprehensive Overview of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/9FDXZQ2V/2307.html:text/html},
}

@article{wang_commonsensevis_2023,
	title = {{CommonsenseVIS}: {Visualizing} and {Understanding} {Commonsense} {Reasoning} {Capabilities} of {Natural} {Language} {Models}},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{CommonsenseVIS}},
	url = {https://ieeexplore.ieee.org/document/10297594/},
	doi = {10.1109/TVCG.2023.3327153},
	abstract = {Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models’ implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models’ relational reasoning over concepts in different situations.},
	language = {en},
	urldate = {2023-12-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, Xingbo and Huang, Renfei and Jin, Zhihua and Fang, Tianqing and Qu, Huamin},
	year = {2023},
	pages = {1--11},
	file = {Wang et al. - 2023 - CommonsenseVIS Visualizing and Understanding Comm.pdf:/Users/ninalei/Zotero/storage/CKVI4QYN/Wang et al. - 2023 - CommonsenseVIS Visualizing and Understanding Comm.pdf:application/pdf},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/CUT287ZM/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/SW7AADG9/2210.html:text/html},
}

@misc{bosma_introducing_2021,
	title = {Introducing {FLAN}: {More} generalizable {Language} {Models} with {Instruction} {Fine}-{Tuning}},
	shorttitle = {Introducing {FLAN}},
	url = {https://blog.research.google/2021/10/introducing-flan-more-generalizable.html},
	language = {en},
	urldate = {2023-12-13},
	author = {Bosma, Maarten},
	month = oct,
	year = {2021},
	file = {Snapshot:/Users/ninalei/Zotero/storage/AUA2F2ZW/introducing-flan-more-generalizable.html:text/html},
}

@misc{bosselut_comet_2019,
	title = {{COMET}: {Commonsense} {Transformers} for {Automatic} {Knowledge} {Graph} {Construction}},
	shorttitle = {{COMET}},
	url = {http://arxiv.org/abs/1906.05317},
	doi = {10.48550/arXiv.1906.05317},
	abstract = {We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5\% (ATOMIC) and 91.7\% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
	month = jun,
	year = {2019},
	note = {arXiv:1906.05317 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted to ACL 2019},
	file = {arXiv Fulltext PDF:/Users/ninalei/Zotero/storage/MNFN6WX3/Bosselut et al. - 2019 - COMET Commonsense Transformers for Automatic Know.pdf:application/pdf;arXiv.org Snapshot:/Users/ninalei/Zotero/storage/PLAL7D6X/1906.html:text/html},
}

@misc{sap_atomic_2019,
	title = {{ATOMIC}: {An} {Atlas} of {Machine} {Commonsense} for {If}-{Then} {Reasoning}},
	shorttitle = {{ATOMIC}},
	url = {http://arxiv.org/abs/1811.00146},
	abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., "if X pays Y a compliment, then Y will likely return the compliment"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
	month = feb,
	year = {2019},
	note = {arXiv:1811.00146 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AAAI 2019 CR},
	file = {arXiv.org Snapshot:/Users/ninalei/Zotero/storage/ZXTZLN2Q/1811.html:text/html;Full Text PDF:/Users/ninalei/Zotero/storage/QEWMGSSB/Sap et al. - 2019 - ATOMIC An Atlas of Machine Commonsense for If-The.pdf:application/pdf},
}
